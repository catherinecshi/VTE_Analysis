BP07 Day6 is incomplete and doesn't follow the format my current code for getting ss logs
    it doesn't have a csv file but has a ss logs
Same for BP07 Day10, except the ss log follows the format for my code
    but no csv file
TH508 Day 15 - more than one DLC file


benefits of tracking the body instead of head for initial analysis of where the rat is in space

filtered out based on likelihood first, because i assume the jumps shouldn't include coords with low likelihood

do framerates for occupancy again

i need to find a better way to do the gridding when generating lines

BP13 - day1, day 7, day 14, day 15, day 16, day 18, day 19, day 24, day 29
TH605 - day1, day 3, day 5, day 6, day 7, day 9, day 10, day 19, day 29
BP15 - day 1, day 4, day 8, day 9, day 11, day 13, day 14, day 21, day 27
BP16 - day 1, day 5, day 9, day 10, day 11, day 12, day 13, day 16, day 19
BP22 - day 1, day 4, day 7, day 8, day 9, day 10, day 11, day 21, day 50
BP21 - day 1, day 6, day 11, day 12, day 19, day 26, day 27, day, day 53
BP19

remember how many rats in my data set

need to sort out days 42-45 for bp11
need to sort out day 31 for TH405
bp19 day 1 wrong on spreadsheet

bp15 day 13
th508 day31
th508 day27
th405 day31

im using 11 rats in my data

main problems
- weird addition of tottal (sometimes correct) into all trials near the start? - not fixed
- sometimes statescript is weird and it'll have an extra middle numbers ( should be fixed)
- i don't think i'm dealing with it ending very well - look at TH605 day5, i get 19/25 BC but reality is 20/25 BC bc it ends on BC
- not having starting commens (should be fixed)
- some skipped days - print out days when they're skipped in the future

maybe i should try to cut off the lines when they don't touch a dot in x number of segments

filter such that the start of the DLC matches the start of the SS
maybe some form of filtering such that if one point juts out (like the diff between that point and the two
surrounding points is high, but the diff between the two points without that point is low)

do something about when not enough lines for intersection, rn threshold is being hard set

next - make lines better at the start -> i think this will help with the ocnvex hull creation

right now i'm just averaging zIdPhi & IdPhi values regardless of trial type, should probs do according to tiral type at some point

cdf plot for the std




code comments, line numbers are from github.
Starting line 65: if fnmatch.fnmatch(f, '*dlc*.csv'):
Overall this isn't the best way to handle it since there can be 2 (or more) DLC files and they are all good. For the full data set, there will be sleep DLCs. However, even if you ignore those, there may be 2 or more DLC's for the track if Trodes/Statescript crashed. These will need to be concatenated together.
Starting line 78: if fnmatch.fnmatch(f, '*track*.statescriptlog'): basically the same. There will be multiple SS files if trodes/SS crashed and restarted.
Lines 171, 174:  having values, especially ones that are criteria/thresholds for things, is always better to make a variable and have it set at the top of the program in a section called something like user inputs. This is good for a few reasons. When you get to having thousands of lines, there is no way you'll remember the 3 (line 171) or 0.999 (line 174) is set here or all the other little "hard coded" choices you've made. Having it, and all other values, a declared variable at the top of the program ensures you, and most importantly anyone who uses your code in the future, knows this is a value being used and they can easily change it to suit their needs / iterate testing.
Line 171 filter_dataFrame: Few notes. Function naming here is quite vague. This function really has to do with detecting and fixing tracking jumps. However, "filter_dataframe" is super vague and doesn't convert they is a tracking cleaning function.
Otherwise, im surprised this method works. As far as I can tell this could only detect a jump in a single frame. Say tracking goes from the greenLED to a spot on the floor for 1 second then back to the greenLED. Wont line 186 only find jumps on the frame it jumps to the ground and the frame it jumps back to the greenLED? And what is happening to all the data points that are <= 0.999? Are they getting dropped?
Line 200 get_time: Also not sure how this will handle multiple recs if SS/Trodes crashed, esp with line 214 -starting_time.
Also, wouldn't it be a lot better to store starting_time rather than search for it every time get_time is called?
Line 219: calculate_framerate Using this method to calculate time can develop errors. There are often dropped frames, so the time between two frames could be framerate*2 or *3 or more. Furthermore there can be some stutter in frames at the start and ends of recordings. Its okay as a heuristic and probably for behavior metrics, but keep it in mind. This can also be a nice example where a unit test comes in. Unit testing is where you test the output of each function, which makes debugging 100x easier and quicker. For example, if you had a list of all the times from SS log, you can do .diff and mean and see how close 1/framerate is to the mean .diff. Then do something like if abs of different is > 0.1, throw error.

Keep in mind that using print() a lot will slow down your code. It's great for debugging but if you dont need it then its best to get rid of. For example theres a lot of prints in line 257: trial_analysis
I honestly don't know what is best practice for the process of parsing the statescript. When I do it, I get everything out in the same function, that way I only loop through the SS log one time. You have trial_analysis, get_trial_types, and time_until_choice that are all looping through the whole SS log. It's at least faster if you looped through the log once and called three different functions inside the big loop to get out the needed data.
Line 373 and others: Home may not always be arm 1, best to make 1 a variable
You call filter_dataframe 4 times, but it's all on the same DLC data. Isn't it a lot better to just save the output once instead of calling it 4 times? On that note, and im plenty guilty of this too, but do check your variable naming over time. You call the same data from DLC, "DLC" "DLC_df" and "DLC_data"





remember to cut everything down to 3sf or no decimal places

next on agenda:
- make sure zIdPhi values get corresponded with trajectories
- determine what are VTEs and what aren't
- fix based on comments above

- print something if there are less than 10 trials and run an error or warning
- BP07 got retrained, so nothing above day 28 reaaally
- BP13 day 19 analyse as two separate session essentially
    - not for learning rates tho
- BP22 Day47 check if timestamps reset between 1 and 2
    - if it reset, find the offset and add it
    - also bp15 day12 13
    - bp 19 day 28 as well
- BP21 jenya put the wrong rat on, so check the ss and cut out when new rat is put on
    - take last timestamp of first dlc and cut ss there
- make ss logs dataframes instead

rn for days_since_new_arm it counts trials when there's a decrease in trials
but that doesn't correspond to the 'trial it is learning'.... fix in future? cut it off?