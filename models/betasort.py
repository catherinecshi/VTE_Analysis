import numpy as np

# pylint: disable=global-statement, logging-fstring-interpolation, consider-using-enumerate

class Betasort:
    """
    RL model specific to Transitive Inference according to pseudocode from Jensen 2015 paper.
    
    Description:
    Each stimulus in TI is represented with a beta distribution, with the upper (U) parameter
    corresponding to the alpha value, and the lower (L) parameter corresponding to the beta
    value. This means:
        - each distribution is bounded from 0 to 1 (in the TI hierarchy)
        - if U & L are low -> high uncertainty (and vice versa)
        - increasing U or L conceptually means "more information" was fed into the model
        - U > L -> closer to 1 (higher in hierarchy), and vice versa
    
    The Reward (R) & Non-reward (N) parameters correspond to reward and non-reward history.
    It is used to check the rate at which a stimulus has been rewarded, which is used to
    calculate the additional amount U & L parameters should be depreciated. Thus, high
    reward rate (recent) -> smaller updates.
    
    U & L parameters can be updated using the update() learning algorithm.
    choose() is the decision policy determining which of two presented stimuli will be chosen.
    
    Parameters: 
        - tau : float
            - 0 < tau < 1
            - noise parameter
            - higher -> more likely to choose by random
        - xi : float
            - 0 < xi < 1
            - recall parameter
            - lower -> parameters depreciate faster
    
    Stored History (stored every trial):
        - uncertainty_history
            - uncertainty of each stimulus
        - ROC_uncertainty_history
            - uncertainty of each pair of stimuli
        - position_history
            - mean of beta distribution for each stimulus
        - U_history
        - L_history
    """
    
    def __init__(self, n_stimuli, rat, day, tau=0.05, xi=0.95):
        """
        initialize all parameters, memory arrays, and history arrays
        """
        # initialize parameters
        self.n_stimuli = n_stimuli
        self.rat = rat
        self.day = day
        self.trial = 0
        self.tau = tau
        self.xi = xi

        # initialize memory arrays
        self.U = np.ones(n_stimuli) # upper parameter
        self.L = np.ones(n_stimuli) # lower parameter
        self.R = np.ones(n_stimuli) # rewards
        self.N = np.ones(n_stimuli) # non rewards
        
        # store history for later plotting
        self.uncertainty_history = [self.get_all_stimulus_uncertainties()]
        self.ROC_uncertainty_history = [self.get_all_ROC_uncertainties()]
        self.position_history = [self.get_all_positions()]
        self.U_history = [self.U.copy()]
        self.L_history = [self.L.copy()]
    
    def choose(self, available_stimuli):
        """
        selects a stimulus from presented stimuli in a given trial
        
        Description:
        A random number is sampled from the beta distribution of each stimulus, and
        the stimulus with the largest number is the chosen stimulus. The noise
        parameter determines how likely an entirely random value is chosen.
        
        Parameters:
            - available_stimuli : int array
                - array of the two presented stimuli
                - the int corresponds to index in overall list of stimuli
        """
        
        # array to store the random value generated by each stimulus
        X = np.zeros(len(available_stimuli))
        
        for i, stim_idx in enumerate(available_stimuli):
            if np.random.random() < self.tau: # choose randomly
                X[i] = np.random.beta(1, 1)
            else: # base off of learned stimuli beta distributions
                # + 1 to prevent values being 0
                X[i] = np.random.beta(self.U[stim_idx] + 1, self.L[stim_idx] + 1)
        
        chosen_idx = np.argmax(X) # choose stimulus with largest value
        return available_stimuli[chosen_idx]
    
    def update(self, chosen, unchosen, reward, probability, threshold):
        """
        learning algorithm for updating memory vectors (U, L, R, N)
        
        Description:
        The learning algorithm has three stages
        1. relaxation stage
            - memory vectors depreciated by the recall rate (xi)
            - U and L also depreciated by the reward-modulated recall rate (xi_R)
                - so, if already lots of recent reward -> less updates
        2. explicit feedback stage
            - the match rate between model choice and real choice -> probability
            - if reward but match rate < threshold
                - U of chosen stimulus (Uch) & L of the unchosen(Luc) + 1 
                - R chosen & U unchosen + 1
            - if reward and match rate >= threshold
                - U and L parameters consolidated using trial reward rate (V)
                    - V represents the “expected” values of each stimulus
                - R chosen & U unchosen + 1
            - if no reward
                - N chosen and N unchosen + 1
                - U unchosen & L chosen + 1
        3. implicit inference stage (only if no reward)
            - conceptually, this is when the rest of the hierarchy adapts to explicit feedback updates
            - stimuli lower than unchosen -> decrease (increase L)
            - stimuli higher than chosen -> increase (increase U)
            - between chosen & unchosen -> consolidate (increase U & L according to expected value)
        
        After updates, values are stored in the history arrays.
        
        Parameters:
            - chosen : int
                - index of chosen stimulus
            - unchosen : int
                - index of unchosen stimulus
            - reward : int
                - 1 for reward and 0 otherwise
            - probability : float
                - probability of how much the simulated data matches up with the real choice
                - used to update the model proportionally
            - threshold : float
                - threshold for above which consolidation is done
        """
        self.trial += 1
        
        # relax parameters
        self.R = self.R * self.xi
        self.N = self.N * self.xi
        
        # estimate trial reward rate
        E = self.R / (self.R + self.N)
        xi_R = E / (E + 1) + 0.5
        
        # relax some more :)
        self.U = self.U * xi_R * self.xi
        self.L = self.L * xi_R * self.xi

        # estimated value of each stimulus
        V = self.U / (self.U + self.L)

        # explicit feedback
        if reward == 1 and probability < threshold: # if correct trial + choice did not match data
            # update reward rate
            self.R[unchosen] = self.R[unchosen] + 1
            self.R[chosen] = self.R[chosen] + 1
            
            # shifts chosen up, unchosen down
            self.U[chosen] = self.U[chosen] + 1
            self.L[unchosen] = self.L[unchosen] + 1
        elif reward == 1: # correct trial + choice matched data
            self.R[unchosen] = self.R[unchosen] + 1
            self.R[chosen] = self.R[chosen] + 1
            
            # shifts chosen up, unchosen down
            self.U = self.U + V
            self.L = self.L + (1 - V)
        else: # incorrect trial
            # update reward rate
            self.N[unchosen] = self.N[unchosen] + 1
            self.N[chosen] = self.N[chosen] + 1
            
            # shift unchosen up, chosen down
            self.U[unchosen] = self.U[unchosen] + 1
            self.L[chosen] = self.L[chosen] + 1
            
            # process other stimuli (implicit inference)
            for j in range(self.n_stimuli):
                if j != chosen and j != unchosen:
                    if V[j] > V[chosen] and V[j] < V[unchosen]:
                        # j fell between chosen and unchosen (consolidate)
                        self.U[j] = self.U[j] + V[j]
                        self.L[j] = self.L[j] + (1 - V[j])
                    elif V[j] < V[unchosen]:
                        # shift j down
                        self.L[j] = self.L[j] + 1
                    elif V[j] > V[chosen]:
                        # shift j up
                        self.U[j] = self.U[j] + 1

        # store updated uncertainties and positions upper and lower
        self.uncertainty_history.append(self.get_all_stimulus_uncertainties())
        self.ROC_uncertainty_history.append(self.get_all_ROC_uncertainties())
        self.position_history.append(self.get_all_positions())
        self.U_history.append(self.U.copy())
        self.L_history.append(self.L.copy())

    def get_uncertainty_stimulus(self, stimulus_idx):
        """
        calculates uncertainty for a given stimulus using variance of beta distribution
        
        Parameters:
            - stimulus_idx (int): index of stimulus (0 is A, 1 is B, etc.)
        
        Returns:
            - (float): uncertainty value
        """
        
        a = self.U[stimulus_idx]
        b = self.L[stimulus_idx]
        
        # avoid division by zero or undefined values
        if a + b < 2:
            return 1.0 # maximum uncertainty
        
        return (a * b) / ((a + b)**2 * (a + b + 1))

    def get_all_stimulus_uncertainties(self):
        """Get uncertainty values for all stimuli"""
        return np.array([self.get_uncertainty_stimulus(i) for i in range(self.n_stimuli)])
    
    def get_uncertainty_relation(self, chosen_idx, other_idx, n_samples=1000):
        """
        Calculates the uncertainty about the relationship between two stimuli
        
        Parameters:
            - chosen_idx (int): index of first stimulus
            - other_idx (int): index of second stimulus
            - n_samples (int): number of Monte Carlo samples
            
        Returns:
            - (float): uncertainty value between 0 and 1 (0=certain, 1=maximally uncertain)
        """
        # Get Beta distribution parameters
        a1 = self.U[chosen_idx] + 1
        b1 = self.L[chosen_idx] + 1
        a2 = self.U[other_idx] + 1
        b2 = self.L[other_idx] + 1
        
        # Generate samples from both distributions
        samples1 = np.random.beta(a1, b1, n_samples)
        samples2 = np.random.beta(a2, b2, n_samples)
        
        # Calculate probability that samples1 > samples2
        prob_greater = np.mean(samples1 > samples2)
        
        # Convert to uncertainty measure (0 = certain, 1 = maximally uncertain)
        uncertainty = 1 - 2 * abs(prob_greater - 0.5)
        
        return uncertainty
    
    def get_all_relational_uncertainties(self):
        """get probabilistic uncertainties for all adjacent pairs of stimulus"""
        return np.array([self.get_uncertainty_relation(i, i+1) for i in range(self.n_stimuli-1)])

    def get_uncertainty_relation_ROC(self, chosen_idx, other_idx, n_samples=10000):
        """
        Calculates the uncertainty about the relationship between two stimuli using ROC analysis
        
        Parameters:
            - stimulus_idx1 (int): index of first stimulus
            - stimulus_idx2 (int): index of second stimulus
            - n_samples (int): number of samples for ROC calculation
            
        Returns:
            - (float): uncertainty value between 0 and 1 (0=certain, 1=maximally uncertain)
        """
        # Get Beta distribution parameters
        a1 = self.U[chosen_idx] + 1
        b1 = self.L[chosen_idx] + 1
        a2 = self.U[other_idx] + 1
        b2 = self.L[other_idx] + 1
        
        # Generate samples from both distributions
        samples1 = np.random.beta(a1, b1, n_samples)
        samples2 = np.random.beta(a2, b2, n_samples)
        
        # Calculate AUC (Area Under the ROC Curve)
        # This is equivalent to the probability that a randomly selected
        # value from samples1 exceeds a randomly selected value from samples2
        auc = 0
        for i in range(n_samples):
            auc = np.mean(samples1 > samples2)
        auc /= n_samples
        
        # Convert AUC to uncertainty measure
        # AUC of 0.5 represents maximum uncertainty (distributions completely overlap)
        # AUC of 0 or 1 represents minimum uncertainty (distributions completely separated)
        uncertainty = 1 - 2 * abs(auc - 0.5)
        
        return uncertainty
    
    def get_all_ROC_uncertainties(self):
        """get probabilistic uncertainties for all adjacent pairs of stimulus"""
        return np.array([self.get_uncertainty_relation_ROC(i, i+1) for i in range(self.n_stimuli-1)])
    
    def get_all_positions(self):
        """Get estimated positions for all stimuli"""
        positions = np.zeros(self.n_stimuli)
        for i in range(self.n_stimuli):
            if self.U[i] + self.L[i] == 0:
                positions[i] = 0.5
            else:
                positions[i] = self.U[i] / (self.U[i] + self.L[i])
        return positions
