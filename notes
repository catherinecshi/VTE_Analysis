### PANDAS NOTES
- Series creates an enumerated one dimensional array of whatever you feed into it
    - .index = [] to change the index of the series to something arbitrary
    - series are ordered, and so are not like dictionaries in that way
    - the upper limit is included
- Dataframe is a type of 2D array in pandas, and can be created in many different ways
    - can be created by passing a dictionary of objects in
    - keys are the column labels and the values are the column values
    - although the keys are the column labels, the rows are usually just labelled through simple enumeration, starting from 0
    - can have different types of enumeration
        - for example, pd.date_range([date], [period]) creates a 2D array with a datetime index
        - like 2013-01-01 can be fed where the [date] is, and the row labels will start with that date
    - each column must only contain values pertaining to one dtype
    - can assign the order of the dataframe using columns=[]
        - so this would be the order of the headers
    - can also reassign the index of the dataframe using df.index()
        - this is the row heading (so going down vertically)

### Pandas Common Commands
- df.info()
- viewing data
    - everything can be used as df instead, so DataFrame.head() can be used through typing df.head() instead
    - DataFrame.head() & DataFrame.tail()
        - view the top and bottom rows of the frame
    - DataFrame.index
        - shows the index (row labels), the dtype, and the freq
    - DataFrame.columns
        - shows the column labels and the dtype
    - DataFrame.to_numpy()
        - returns numpy representation without index or clumn labels
        - careful because numpy arrays can only have one dtype for the entire array
    - DataFrame.describe()
    - DataFrame.T
    - DataFrame.sort_index()
        - sorts by an axis and ascending/descending
    - DataFrame-sort_values()
        - sorts by a specific column
    - df.size()
    - df.shape()
- selecting data
    - pass a single label through DataFrame
        - e.g. df["A"]
            - turns that column & index into a Series
        - can select multiple things as well
            e.g. df[["A", "B"]]
    - slice with a colon
        - e.g. df[0:3] returns 0th row (column labels) to the 3rd row
    - more recommended to use loc and iloc instead of direct slicing
        - for row level selections
        - .loc[]
            - selects rows matching the given index
            - can also use to slice
                - e.g. df.loc['France': 'Italy']
                - this includes france, italy, and anything in between when slicing
            - can also select the columns you want to see
                - e.g. df.loc['France':'Italy', 'Population']
        - .iloc[]
            - works with the numeric position of the index
            - so even when indices have been assigned, you can use numbers that correspond to the correct row
            - nice for when you want to select the last row for example -> df.iloc[-1]
            - but something like df.iloc[1:3] will only include [1] and [2], not [3]
    - in all of these examples, 'df' is the variable name
- multiindex
    - e.g. multi_index = pd.MultiIndex.from_tuples([])
    

### Pandas Notes
- Conditional Statements/Boolean Arrays
    - e.g. df['Population'] > 70
    - this will return an array of false and trues, each corresponding to its 'index'
    - and if you use df.loc[df['Population'] > 70], you can display the ones that are true
    - can also df.loc[df['Population'] > 70, 'Population']
- Dropping
    - point to the values you want to drop
    - df.drop(index)
    - df.drop(columns=[])
- Operations
    - df[['Population', 'GDP']] / 100
- modifying dataframes
    - add columns
        - df[new column header] = series
        - e.g. df['Language'] = langs
    - replace column values
        - e.g. df['Language'] = 'English'
    - renaming columns
        - e.g. df.rename()
    - adding values
        - df.append(series)
        - where the series you give in append include the values and the column it corresponds to
            - e.g. df.append(pd.Series({
                'Population': 3,
                'GDP': 5
            }, name = 'China'))
        - and any values not included will just be NaN
    - directly set the idnex and values
        - e.g. df.loc['China'] = pd.Series({'Population': 1_400_000_000, 'Continent': 'Asia'})


### DATA CLEANING NOTES
- Falsy
    - values like 0, False, None, '', [], {}
- np.nan
    - nullable value - not a number
    - everything it touches becomes np.nan
- np.inf
    - also a virus like np.nan
- can check for nans or infs with np.isnan or np.isinf respectively
    - or jointly, np.isfinite
- can filter them out
    - e.g. a = np.array([1, 2, 3, np.nan, np.nan, 4])
           a[~np.isnan(a)]
           # this is the same as a[np.isfinite(a)]
    - this allows things like .sum() and .mean() to be carried out without turning into nans
- identifying and detecting null variables
    - pd.isnull
        - true for np.nan
        - true for None
    - pd.isna
        - true for np.nan & None
    - pd.notnull
    - pd.notna
    - these return a boolean array when working with series and dataframes
    - .dropna()
        - drops the NaN values, keepign the indices
        - meaning if 3 and 4 had NaN, after dropna, 3 and 4 will still be gone
- when using things like .sum() with .null()
    - returns the sum of 'true's
    - e.g. s = pd.Series([1, 2, 3, np.nan, np.nan, 4])
           s[s.notnull()] returns 
           [0, 1.0], [1, 2.0], [2, 3.0], [5, 4.0]
- dropping values in dataframes
    - if you dropna, it'll just drop any row where there are any null values
    - or alternatively, by .dropna(axis=1), you can drop the columns instead
    - or another alternative, you can choose to drop any or all
        - e.g. df.dropna(how = 'all')
            - this drops rows only if all of the values are Nan
        - default is .dropna(how = 'any')
    - or use thresh to indicate a threshodl of number of non-null values
        - e.g. df.dropna(thresh = 3)
        - this only keeps columsn if it has 3 or more non-null values
- alternatively, instead of dropping null values, you can replace it with another value
    - .fillna
        - e.g. s.fillna(0)
        - e.g. s.fillna(s.mean())
    - .fillna(method = 'ffill')
        - fill null values with other values close to the null
        - ffill fills null values with the value in front of it
            - this works for continuous nan values too
            - so continuous nan values would all become the same value (the one in front of them)
        - bfill fills null values with the value after it
    - all of this still leaves the nan values at the start or last, depending on ffill or bfill
- filling nulls in dataframes
    - .fillna but you can specify the axis to use to fill the values
    - can also specify different values for different columns
- checking if there are NAs
    - method 1: checking length
        - e.g. print(missing_values = s.count() != len(s))
        - .count() doesn't include na
    - method 2: .any() or .all()
        - .any() returns true if there are any trues
        - .all() returns true if all are true
- checking for invalid values
    - such as being given "D" for sex
    - .unique() for categorical type of field
        - returns every unique value
        - for example, for sex, it should be 'M', 'F', or 'Other'
    - .value_counts()
        - returns number of instances for every unique value
    - .replace()
        - e.g. df['Sex'].replace('D', 'F')
        - this replaces all D with F
    - what if you want to remove all the extra 0s from the age coumn
        - df[df['Age'] > 100]
            - this identifies all values over your limit
        - df.loc[df['Age'] > 100, 'Age'] = df.loc[df['Age'] > 100, 'Age'] / 10
            - so the df['Age'] > 100 selects for the rows in which the age > 100
            - and the second 'Age' selects the 'Age' column within those rows
- dealing with duplicates in series
    - .duplicated()
        - shows you which ones are duplicated
        - but only shows you the second+ of the same value, not the first
        - can change this behaviour with the keep parameter - .duplicated(keep = 'last')
            - this will consider the last 'duplicate' as not a duplicate instead
        - or can do .duplicated(keep = False)
            - everything counts as a duplicate
    - .drop_duplicates()
        - same keep rules applies
- dealing with duplicates in dataframe
    - dupicates in df happen at a 'row' level
    - can specify which column to look for
        - e.g. players.duplicated(subset=['Name'])
        - otherwise all columns values have to be dupulicates for it to count
    - same rules of keep
- text handling
    - splitting method
        - e.g. df['Data'].str.split('_')
            - this splits when there is an underscore
            - still same column tho
        - expand = True
            - creates multiple columns
    - .contains()
    - .strip()
        - removes blank spaces


### READING & SAVING FILES NOTES
- first thing to do -> open()
    - takes path to the file
    - returns the file object
    - by using the [with] statement, it'll automatically close the file even during errors
        - with open(filepath, 'r') as reader:
                print(reader)
    - then can read the contents
        - for index, line in enumerate(reader.readlines()):
                #reading just the first 10 lines
                if (index < 10):
                    print(index, line)
- has specialised functions for different files it could read
- read_csv
    - takes filepath
    - you can let pandas infer everything, or explicitly tell it how to load the data using parameters
    - header
        - usually it'll take the first row as the header
        - indicate otherwise using header
        - can manually indicate headers with names = []
    - na_values
        - na_values = []
        - indicate what to be recognised as NaN
    - dtype
        - takes dictionary of column + dtype
    - sep
        - indicate which characters are 'separaters'
        - and can separate things into different columns
    - etc.
- saving a csv file
    - e.g. exam_df.to_csv()
        - can specify file name or path
    
    
### OS NOTES
- os.path
    - pathname manipulations
    - os.path.join()
        - joins path componenents with '/' after each non-empty part (except last one)
- list of methods within os
    - print(dir(os))
- os.getcwd()
    - current working directory
- os.listdir()
    - list the folders/files in current directory
- os.walk()
    - iterate over all the files in each folder of the current directory
    - to list all the files
    - def list_files(startpath):
        for root, dirs, files in os.walk(startpath):
            if dirs != '.git':
                level = root.replace(startpath, '').count(os.sep)
                indent = ' ' * 4 * (level)
                print('{}{}/'.format(indent, os.path.basename(root)))
                subindent = ' ' * 4 * (level + 1)
                for f in files:
                    print('{}{}'.format(subindent, f))
- os.chdir()
    - change directories
    - e.g. os.chdir('../')
        - this moves up in the directory structure
- os.mkdir()
    - make directory
    - cannot make two at the same time
        - use os.mkdirs() for that
- os.rmdir()
    - remove directories
    - cannot remove directories unless it is empty
